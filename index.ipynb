{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and Configuring PySpark with Docker\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In a large-scale enterprise environment, you would typically run (Py)Spark on a distributed cloud system or possibly a dedicated hardware in a datacenter. However it is also possible to run it on your local computer as a standalone cluster. In this lesson we'll walk through how to do just that!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Understand how to install PySpark on your local computer\n",
    "- Explain the utility of Docker when dealing with package management \n",
    "- Install a Docker container that comes packaged with Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PySpark without Docker\n",
    "\n",
    "It is possible to install PySpark without Docker, but it will require more-advanced systems administration skills. Follow these instructions if you want to get the best possible Spark performance from your personal computer, but be aware that **you can feel free to skip this** because there are a lot of little places that things can go wrong, and it can be difficult to troubleshoot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a New `conda` Environment\n",
    "\n",
    "If you want to work on a project using PySpark, we recommend that you make a new `conda` environment. Execute the following commands in the terminal:\n",
    "\n",
    "```bash\n",
    "conda activate base\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda create --name spark-env python=3.8\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda activate spark-env\n",
    "```\n",
    "\n",
    "#### Checking for Successful Environment Creation\n",
    "\n",
    "Run this command in the terminal:\n",
    "\n",
    "```bash\n",
    "which python\n",
    "```\n",
    "\n",
    "Make sure that the path displayed includes `spark-env` before proceeding to the next step. If it doesn't, try `conda deactivate` repeatedly until no environment is shown, then `conda activate spark-env` again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Java\n",
    "\n",
    "With `spark-env` activated, execute this line in the terminal:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge openjdk=11\n",
    "```\n",
    "\n",
    "This installs OpenJDK, an open-source version of Java, within your `conda` environment.\n",
    "\n",
    "#### Checking for Successful Java Installation\n",
    "\n",
    "Run this command in the terminal:\n",
    "\n",
    "```bash\n",
    "which java\n",
    "```\n",
    "\n",
    "Make sure the path displayed includes `spark-env`.\n",
    "\n",
    "Launch an interactive **Java shell** by running:\n",
    "\n",
    "```bash\n",
    "jshell\n",
    "```\n",
    "\n",
    "This should launch a CLI application that displays a `jshell>` prompt.\n",
    "\n",
    "If you want to write some Java code here, you can! Or you can just quit the Java shell by typing `/exit` and hitting Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing PySpark\n",
    "\n",
    "With `spark-env` activated, execute this line in the terminal:\n",
    "\n",
    "```bash\n",
    "pip install pyspark==3\n",
    "```\n",
    "\n",
    "This installs a setup for a standalone Spark cluster as well as the PySpark library to interact with that cluster.\n",
    "\n",
    "#### Checking for Successful PySpark Installation: Spark\n",
    "\n",
    "First, check that Spark installed successfully by launching an interactive **Spark shell**:\n",
    "\n",
    "```bash\n",
    "spark-shell\n",
    "```\n",
    "\n",
    "This should launch a CLI application that displays a `scala>` prompt. It's normal for this to take several seconds, and also to print out several lines of warnings, such as `WARNING: Illegal reflective access`, as well as some fun ASCII art:\n",
    "\n",
    "```\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\\n",
    "      /_/\n",
    "```\n",
    "\n",
    "Try typing `sc` and hitting Enter to see the string representation of the SparkContext in Scala. Then quit the Scala shell by typing `:quit` and hitting Enter.\n",
    "\n",
    "#### Checking for Successful PySpark Installation: PySpark\n",
    "\n",
    "Then, check that PySpark installed successfully by launching an interactive **PySpark shell**. Run this command in the terminal:\n",
    "\n",
    "```bash\n",
    "pyspark\n",
    "```\n",
    "\n",
    "This should launch a CLI application that displays a `>>>` prompt, as well as the same warnings and ASCII art as the `spark-shell` did. Try typing `sc` and hitting Enter to see the string representation of the SparkContext in Python (and you can test out any other random Python commands). Then quit the PySpark shell by typing `quit()` and hitting Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Jupyter Notebook and Other Useful Libraries\n",
    "\n",
    "In theory `spark-env` already has everything you need to start developing PySpark code! But there are a couple more tools that we typically use for data science that you probably want to install as well.\n",
    "\n",
    "With `spark-env` activated, run these commands in the terminal to install Jupyter Notebook and Matplotlib. Afterwards you can install any other libraries you like to use as well.\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge notebook\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name spark-env --display-name \"Python (spark-env)\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda install matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a PySpark Notebook Locally\n",
    "\n",
    "Now you can clone this notebook and run `jupyter notebook` to start it in your `spark-env`. The three cells below should run with no errors (although there may be warnings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/07 20:38:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[597, 803, 304, 458, 603]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Installing PySpark with Docker\n",
    "\n",
    "If you're lucky, all of the commands above ran smoothly and you are now able to use PySpark directly on your machine using `conda`. However we recognize that this process frequently goes wrong, and troubleshooting often involves some tricky systems administration tasks, configuring environment variables such as `$PATH`, `$JAVA_PATH`, etc.\n",
    "\n",
    "In the rest of this lesson, we'll describe an alternative way to install PySpark using **Docker**.\n",
    "\n",
    "### Why Docker?\n",
    "\n",
    "Docker is a container technology that allows __packaging__ and __distribution__ of software  so that it takes away the headache of things like setting up an environment, configuring logging, configuring options, etc. Docker basically removes the excuse of *It doesn't work on my machine*.\n",
    "\n",
    "[Visit this link learn more about docker and containers](https://www.zdnet.com/article/what-is-docker-and-why-is-it-so-darn-popular/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does Docker Work?\n",
    "\n",
    "Without getting too much into the details of virtualization and underlying operating systems, a Docker **image** is deployed in a **container** where it essentially operates as a self-contained computer wherever it is run. It can be running on a Windows desktop, a Mac laptop, or an AWS cloud server, and the dependencies and environment should work exactly the same.\n",
    "\n",
    "Kind of like the `.yml` file we used to create `learn-env`, Docker containers have a static specification that tells the software what to install. Only instead of just Python packages, the **Dockerfile** (as it's called) specifies the operating system, shell language, permissions, environment variables, etc.\n",
    "\n",
    "The Dockerfile we'll be using is for an image maintained by Jupyter called `pyspark-notebook`. You can view the full Dockerfile [here](https://github.com/jupyter/docker-stacks/blob/master/pyspark-notebook/Dockerfile).\n",
    "\n",
    "In order to turn that file into a functioning container, we need to install Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Docker\n",
    "\n",
    "Go to the [Get Docker](https://docs.docker.com/get-docker/) page, click on your operating system, and follow the instructions. Note that there is a graphical user interface called \"Docker Desktop\" available for Mac and Windows users, whereas at the time of this writing there is not an equivalent tool for Linux users. Linux users can install the \"server\" version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the PySpark Stack from DockerHub\n",
    "\n",
    "If you were developing your own Dockerfiles, you could just work locally, similarly to how you could write Python code locally without connecting to any remote repositories. But we want to run an image created by someone else, so we want to use the `docker pull` command from [DockerHub](https://hub.docker.com/r/jupyter/pyspark-notebook). This is roughly equivalent to running `git pull` from GitHub, except you're downloading a pre-built computer image.\n",
    "\n",
    "Specifically, run this command in the terminal:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "This will initiate a download that will likely take a while, then finally you should see a message like this:\n",
    "\n",
    "```\n",
    "Status: Downloaded newer image for jupyter/pyspark-notebook:latest\n",
    "```\n",
    "\n",
    "You have now pulled down the PySpark stack!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Jupyter Notebook with Docker\n",
    "\n",
    "Now that you have pulled down `pyspark-notebook`, run this command in the terminal:\n",
    "\n",
    "```bash\n",
    "docker run -p 8888:8888 jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "(The `-p` flag is setting the ports on your computer as well as the container to be connected.)\n",
    "\n",
    "This will launch a notebook server that should look fairly similar to when you run a regular `jupyter notebook` command!\n",
    "\n",
    "However you will most likely need to copy the URL displayed in the terminal and paste it into a browser window, rather than having it automatically open like `jupyter notebook` usually does. The URL will look something like this:\n",
    "\n",
    "```\n",
    "http://127.0.0.1:8888/lab?token=<token>\n",
    "```\n",
    "\n",
    "Then once you paste it into the browser address bar, you'll be redirected to just `http://127.0.0.1:8888/lab`, which is a Jupyter Lab interface:\n",
    "\n",
    "![jupyter lab screenshot](https://github.com/learn-co-curriculum/dsc-spark-docker-installation/raw/master/images/jupyter_lab.png)\n",
    "\n",
    "If you want to navigate back to the classic `jupyter notebook` file window, simply enter `http://127.0.0.1:8888/tree` in the address bar (replacing `lab` with `tree`).\n",
    "\n",
    "#### Checking for Successful PySpark Image Installation\n",
    "\n",
    "From here, you can create a new notebook and run these lines of code, which should not produce an error:\n",
    "\n",
    "```python\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Docker Container to Your File System\n",
    "\n",
    "You might have noticed something strange when you were creating that notebook: there was only a directory called `work`, nothing related to the directory on your computer where you launched `docker run`!\n",
    "\n",
    "This is because even though the notebook server looked very similar to one being run directly on your computer, this one only had access to the container's file system.\n",
    "\n",
    "If you want to be able to use notebooks from the curriculum or files on disk (e.g. CSV files), it's useful to be able to connect your computer's file system to the container.\n",
    "\n",
    "#### Shutting Down Previous Docker Container\n",
    "\n",
    "Shut down the currently-running container by typing `control-C` in the terminal window where it is currently running. If you accidentally closed that terminal window, you can:\n",
    "\n",
    "1. Use a command-line approach:\n",
    "   * Run `docker ps` to see a list of all currently-running docker containers\n",
    "   * Run `docker stop <container id>` where `<container id>` is from the `docker ps` print-out. For example, `docker stop efb990e0e054`\n",
    "2. Or, use Docker Desktop:\n",
    "   * Open Docker Desktop and locate the currently-running container in the \"Containers / Apps\" list\n",
    "   * Click the square stop button\n",
    "   \n",
    "#### Starting Docker Again, Connected to Your File System\n",
    "\n",
    "The formal language of this is called \"mounting a volume\", so it uses the `-v` command-line option.\n",
    "\n",
    "The general structure looks like this:\n",
    "\n",
    "```bash\n",
    "docker run -p 8888:8888 -v {absolute file path of current directory}:/home/jovyan/work jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "We are mapping `{absolute file path of current directory}` on your computer onto `/home/jovyan/work` in the container.\n",
    "\n",
    "(Fun fact: the username `jovyan` is a [play on the name Jupyter](https://docs.jupyter.org/en/latest/community/content-community.html#what-is-a-jovyan). *Jovyan* is to [*Jovian*](https://en.wiktionary.org/wiki/Jovian) as *Jupyter* is to *Jupiter*.)\n",
    "\n",
    "For **Mac** or **Linux**, the actual command looks like this:\n",
    "\n",
    "```bash\n",
    "docker run -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "For **Windows**, the actual command looks like this (executed in Command Prompt, not Git Bash):\n",
    "\n",
    "```\n",
    "docker run -p 8888:8888 -v %cd%:/home/jovyan/work jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Now you should be able to navigate to the `work` directory and find this notebook there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Couple More Command-Line Options\n",
    "\n",
    "```bash\n",
    "-it\n",
    "```\n",
    "\n",
    "This starts the container in \"interactive mode\" and allows you to access the Bash shell inside the container.\n",
    "\n",
    "```bash\n",
    "--rm\n",
    "```\n",
    "\n",
    "This removes the container from your list of images as soon as you shut it down. Since you are storing your data on your computer's file system, this is a good option to avoid creating a lot of extra unnecessary files.\n",
    "\n",
    "Therefore we recommend that you run this complete command:\n",
    "\n",
    "On **Mac/Linux**:\n",
    "\n",
    "```bash\n",
    "docker run -p 8888:8888 -v $(pwd):/home/jovyan/work -it --rm jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "On **Windows**:\n",
    "\n",
    "```\n",
    "docker run -p 8888:8888 -v %cd%:/home/jovyan/work -it --rm jupyter/pyspark-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson, we looked at installing Spark with and without a Docker container.\n",
    "\n",
    "To recap the steps:\n",
    "\n",
    "### Without Docker\n",
    "\n",
    "Run all of these commands, following the instructions above to ensure that each step worked as expected:\n",
    "\n",
    "```bash\n",
    "conda activate base\n",
    "conda create --name spark-env python=3.8\n",
    "conda activate spark-env\n",
    "conda install -c conda-forge openjdk=11\n",
    "pip install pyspark==3\n",
    "conda install -c conda-forge notebook\n",
    "python -m ipykernel install --user --name spark-env --display-name \"Python (spark-env)\"\n",
    "conda install matplotlib\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### With Docker\n",
    "\n",
    "Go to the [Get Docker](https://docs.docker.com/get-docker/) page, click on your operating system, and follow the instructions.\n",
    "\n",
    "For Mac/Linux:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "docker run -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "For Windows:\n",
    "\n",
    "```\n",
    "docker pull jupyter/pyspark-notebook\n",
    "docker run -p 8888:8888 -v %cd%:/home/jovyan/work -it --rm jupyter/pyspark-notebook\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
